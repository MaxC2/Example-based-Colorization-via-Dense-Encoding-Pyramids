name: "LtoAB"

layer {
  name: "img_l"
  type: "Input"
  top: "img_l"
  input_param {
    shape { dim: 1 dim: 1 dim: 64 dim: 64 }
  }
}

layer {
  name: "ref_ab"
  type: "Input"
  top: "ref_ab"
  input_param {
    shape { dim: 1 dim: 2 dim: 64 dim: 64 }
  }
}

layer { # 0-center data_l channel
  name: "data_l_meansub"
  type: "Scale"
  bottom: "img_l"
  top: "data_l" # [-50,50]
  propagate_down: false
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  scale_param {
    bias_term: True
    filler {      type: 'constant'      value: 1    }
    bias_filler {      type: 'constant'      value: -50    }
  }
}
# **************************************************
# ***** Compute global distribution statistics *****
# **************************************************
layer {
  name: "data_ab_rs"
  type: "Pooling"
  bottom: "ref_ab"
  top: "data_ab_rs"
  pooling_param {
    kernel_size: 4
    stride: 4
    pool: AVE
  }
}
layer { # encode
  type: 'Python'
  name: 'refer_ab_313'
  bottom: "data_ab_rs"
  top: "refer_ab_313" # quantized gt colors
  python_param {
    module: 'caffe_traininglayers'
    layer: 'NNEncLayer'
  }
}
layer {
  name: "refer_glob_ab_313"
  type: "Pooling"
  bottom: "refer_ab_313"
  top: "refer_glob_ab_313"
  pooling_param {
    global_pooling: true
    pool: AVE
  }
}
layer {
  name: "refer_glob_ab_313_drop"
  type: "Python"
  bottom: "refer_glob_ab_313"
  top: "refer_glob_ab_313_drop"
  python_param {
    module: 'caffe_traininglayers'
    layer: 'ColorGlobalDropoutLayer'
    param_str: "1" # keep ratio
  }
}
# seg***************************************
# layer {
#   name: "seg_ab_rs"
#   type: "Pooling"
#   bottom: "seg_data_ab"
#   top: "seg_ab_rs"
#   pooling_param {
#     kernel_size: 4
#     stride: 4
#     pool: AVE
#   }
# }
# layer { # encode
#   type: 'Python'
#   name: 'seg_ab_313'
#   bottom: "seg_ab_rs"
#   top: "seg_ab_313" # quantized gt colors
#   python_param {
#     module: 'caffe_traininglayers'
#     layer: 'NNEncLayer'
#   }
# }
# layer {
#   name: "seg_glob_ab_313"
#   type: "Pooling"
#   bottom: "seg_ab_313"
#   top: "seg_glob_ab_313"
#   pooling_param {
#     global_pooling: true
#     pool: AVE
#   }
# }
# layer {
#   name: "seg_glob_ab_313_drop"
#   type: "Python"
#   bottom: "seg_glob_ab_313"
#   top: "seg_glob_ab_313_drop"
#   python_param {
#     module: 'caffe_traininglayers'
#     layer: 'ColorGlobalDropoutLayer'
#     param_str: "1" # keep ratio
#   }
# }


# ***************************************
# ***** Global distribution network *****
# ***************************************
layer {
  name: "refer_conv1"
  type: "Convolution"
  bottom: "refer_glob_ab_313_drop"
  top: "refer_conv1"
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
  convolution_param {
    num_output: 512
    kernel_size: 1
    weight_filler { type: "gaussian" std: .01 }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
  name: "refer_relu1"
  type: "ReLU"
  bottom: "refer_conv1"
  top: "refer_conv1"
}
layer {
  name: "refer_conv1norm"
  type: "BatchNorm"
  bottom: "refer_conv1"
  top: "refer_conv1norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "refer_conv2"
  type: "Convolution"
  bottom: "refer_conv1norm"
  top: "refer_conv2"
  convolution_param {
    num_output: 512
    kernel_size: 1
    weight_filler { type: "gaussian" std: .01 }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
  name: "refer_relu2"
  type: "ReLU"
  bottom: "refer_conv2"
  top: "refer_conv2"
}
layer {
  name: "refer_conv2norm"
  type: "BatchNorm"
  bottom: "refer_conv2"
  top: "refer_conv2norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "Rrefer_concat1"
  type: "Concat"
  bottom: "refer_conv1norm"
  bottom: "refer_conv2norm"
  top: "Rrefer_concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "refer_conv3"
  type: "Convolution"
  # bottom: "refer_conv2norm"
  bottom:"Rrefer_concat1"
  top: "refer_conv3"
  convolution_param {
    num_output: 512
    kernel_size: 1
    weight_filler { type: "gaussian" std: .01 }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
  name: "refer_relu3"
  type: "ReLU"
  bottom: "refer_conv3"
  top: "refer_conv3"
}
layer {
  name: "refer_conv3norm"
  type: "BatchNorm"
  bottom: "refer_conv3"
  top: "refer_conv3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
layer {
  name: "Rrefer_concat2"
  type: "Concat"
  bottom: "refer_conv1norm"
  bottom: "refer_conv2norm"
  bottom: "refer_conv3norm"
  top: "Rrefer_concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "refer_conv4"
  type: "Convolution"
  bottom: "Rrefer_concat2"
  top: "Rrefer_conv4"
  convolution_param {
    num_output: 512
    kernel_size: 1
    weight_filler { type: "gaussian" std: .01 }
    bias_filler { type: "constant" value: 0 }
  }
}
layer {
  name: "RDBref_RES"
  type: "Eltwise"
  bottom: "refer_conv1norm"
  bottom: "Rrefer_conv4"
  top: "RDBref_RES"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "refer_relu4"
  type: "ReLU"
  # bottom: "refer_conv4"
  bottom:"RDBref_RES"
  top: "refer_conv4"
}
layer {
  name: "refer_conv4norm"
  type: "BatchNorm"
  bottom: "refer_conv4"
  top: "refer_conv4norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# seg***************************************
# layer {
#   name: "seg_conv1"
#   type: "Convolution"
#   bottom: "seg_glob_ab_313_drop"
#   top: "seg_conv1"
#   # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
#   # param {lr_mult: 0 decay_mult: 0} # UNARY_BRANCH_PROPAGATE
#   convolution_param {
#     num_output: 512
#     kernel_size: 1
#     weight_filler { type: "gaussian" std: .01 }
#     bias_filler { type: "constant" value: 0 }
#   }
# }
# layer {
#   name: "seg_relu1"
#   type: "ReLU"
#   bottom: "seg_conv1"
#   top: "seg_conv1"
# }
# layer {
#   name: "seg_conv1norm"
#   type: "BatchNorm"
#   bottom: "seg_conv1"
#   top: "seg_conv1norm"
#   batch_norm_param{ }
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
# }
# layer {
#   name: "seg_conv2"
#   type: "Convolution"
#   bottom: "seg_conv1norm"
#   top: "seg_conv2"
#   convolution_param {
#     num_output: 512
#     kernel_size: 1
#     weight_filler { type: "gaussian" std: .01 }
#     bias_filler { type: "constant" value: 0 }
#   }
# }
# layer {
#   name: "seg_relu2"
#   type: "ReLU"
#   bottom: "seg_conv2"
#   top: "seg_conv2"
# }
# layer {
#   name: "seg_conv2norm"
#   type: "BatchNorm"
#   bottom: "seg_conv2"
#   top: "seg_conv2norm"
#   batch_norm_param{ }
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
# }
# layer {
#   name: "seg_conv3"
#   type: "Convolution"
#   bottom: "seg_conv2norm"
#   top: "seg_conv3"
#   convolution_param {
#     num_output: 512
#     kernel_size: 1
#     weight_filler { type: "gaussian" std: .01 }
#     bias_filler { type: "constant" value: 0 }
#   }
# }
# layer {
#   name: "seg_relu3"
#   type: "ReLU"
#   bottom: "seg_conv3"
#   top: "seg_conv3"
# }
# layer {
#   name: "seg_conv3norm"
#   type: "BatchNorm"
#   bottom: "seg_conv3"
#   top: "seg_conv3norm"
#   batch_norm_param{ }
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
# }
# layer {
#   name: "seg_conv4"
#   type: "Convolution"
#   bottom: "seg_conv3norm"
#   top: "seg_conv4"
#   convolution_param {
#     num_output: 512
#     kernel_size: 1
#     weight_filler { type: "gaussian" std: .01 }
#     bias_filler { type: "constant" value: 0 }
#   }
# }
# layer {
#   name: "seg_relu4"
#   type: "ReLU"
#   bottom: "seg_conv4"
#   top: "seg_conv4"
# }
# layer {
#   name: "seg_conv4norm"
#   type: "BatchNorm"
#   bottom: "seg_conv4"
#   top: "seg_conv4norm"
#   batch_norm_param{ }
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
#   param {lr_mult: 0 decay_mult: 0}
# }



# *****************
# ***** conv1 *****
# *****************
layer {
  name: "bw_conv1_1"
  type: "Convolution"
  bottom: "data_l"
  top: "conv1_1"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "conv1_2norm"
  type: "BatchNorm"
  bottom: "conv1_2"
  top: "conv1_2norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv2 *****
# *****************
layer {
  name: "conv2_1"
  type: "Convolution"
  # bottom: "conv1_2"
  bottom: "conv1_2norm"
  # bottom: "pool1"
  top: "conv2_1"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "conv2_2norm"
  type: "BatchNorm"
  bottom: "conv2_2"
  top: "conv2_2norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv3 *****
# *****************
layer {
  name: "conv3_1"
  type: "Convolution"
  # bottom: "conv2_2"
  bottom: "conv2_2norm"
  # bottom: "pool2"
  top: "conv3_1"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "conv3_3norm"
  type: "BatchNorm"
  bottom: "conv3_3"
  top: "conv3_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv4 *****
# *****************
layer {
  name: "conv4_1"
  type: "Convolution"
  # bottom: "conv3_3"
  bottom: "conv3_3norm"
  # bottom: "pool3"
  top: "conv4_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "conv4_3norm"
  type: "BatchNorm"
  bottom: "conv4_3"
  top: "conv4_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# refer_concat_seg**********************************
# layer {
#   name: "refer_concat_seg"
#   bottom: "refer_conv4norm"
#   bottom: "seg_conv4norm"
#   top: "refer_concat_seg"
#   type: "Concat"
#   concat_param {
#     axis: 1
#   }
# }
layer {
  name: "referseg_conv4norm_rep"
  type: "Python"
  bottom: "refer_conv4norm"
  bottom: "conv4_3norm"
  top: "referseg_conv4norm_rep"
  python_param {
    module: "caffe_traininglayers"
    layer: "SpatialRepLayer"
  }
}

layer {
  name: "seg_conv4_3norm_pglob"
  type: "Eltwise"
  bottom: "conv4_3norm"
  bottom: "referseg_conv4norm_rep"
  top: "conv4_3norm_pseg"
}

# *****************
# ***** conv5 *****
# *****************
layer {
  name: "conv5_1"
  type: "Convolution"
  #bottom: "conv4_3norm"
  # bottom: "pool4"
  bottom: "conv4_3norm_pseg"
  top: "conv5_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "RDB4_conv1"
  type: "Convolution"
  bottom: "conv5_1"
  top: "RDB4_conv1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "RDB4_relu1"
  type: "ReLU"
  bottom: "RDB4_conv1"
  top: "RDB4_conv1"
}
layer {
  name: "RDB4_concat1"
  bottom: "conv5_1"
  bottom: "RDB4_conv1"
  top: "RDB4_concat1"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB4_conv2"
  type: "Convolution"
  bottom: "RDB4_concat1"
  top: "RDB4_conv2"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "RDB4_relu2"
  type: "ReLU"
  bottom: "RDB4_conv2"
  top: "RDB4_conv2"
}
layer {
  name: "RDB4_concat2"
  bottom: "conv5_1"
  bottom: "RDB4_conv1"
  bottom: "RDB4_conv2"
  top: "RDB4_concat2"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB4_conv3"
  type: "Convolution"
  bottom: "RDB4_concat2"
  top: "RDB4_conv3"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "RDB4_relu3"
  type: "ReLU"
  bottom: "RDB4_conv3"
  top: "RDB4_conv3"
}
layer {
  name: "RDB4_concat3"
  bottom: "conv5_1"
  bottom: "RDB4_conv1"
  bottom: "RDB4_conv2"
  bottom: "RDB4_conv3"
  top: "RDB4_concat3"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB4_conv4"
  type: "Convolution"
  bottom: "RDB4_concat3"
  top: "RDB4_conv4"
  convolution_param {
    num_output: 512
    kernel_size: 1
  }
}
layer {
  name: "RDB4_RES"
  type: "Eltwise"
  bottom: "conv5_1"
  bottom: "RDB4_conv4"
  top: "Rconv5_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "Rconv5_1"
  top: "conv5_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    stride: 1
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "conv5_3norm"
  type: "BatchNorm"
  bottom: "conv5_3"
  top: "conv5_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv6 *****
# *****************
layer {
  name: "conv6_1"
  type: "Convolution"
  bottom: "conv5_3norm"
  top: "conv6_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu6_1"
  type: "ReLU"
  bottom: "conv6_1"
  top: "conv6_1"
}

layer {
  name: "RDB5_conv1"
  type: "Convolution"
  bottom: "conv6_1"
  top: "RDB5_conv1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "RDB5_relu1"
  type: "ReLU"
  bottom: "RDB5_conv1"
  top: "RDB5_conv1"
}
layer {
  name: "RDB5_concat1"
  bottom: "conv6_1"
  bottom: "RDB5_conv1"
  top: "RDB5_concat1"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB5_conv2"
  type: "Convolution"
  bottom: "RDB5_concat1"
  top: "RDB5_conv2"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "RDB5_relu2"
  type: "ReLU"
  bottom: "RDB5_conv2"
  top: "RDB5_conv2"
}
layer {
  name: "RDB5_concat2"
  bottom: "conv6_1"
  bottom: "RDB5_conv1"
  bottom: "RDB5_conv2"
  top: "RDB5_concat2"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB5_conv3"
  type: "Convolution"
  bottom: "RDB5_concat2"
  top: "RDB5_conv3"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "RDB5_relu3"
  type: "ReLU"
  bottom: "RDB5_conv3"
  top: "RDB5_conv3"
}
layer {
  name: "RDB5_concat3"
  bottom: "conv6_1"
  bottom: "RDB5_conv1"
  bottom: "RDB5_conv2"
  bottom: "RDB5_conv3"
  top: "RDB5_concat3"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB5_conv4"
  type: "Convolution"
  bottom: "RDB5_concat3"
  top: "RDB5_conv4"
  convolution_param {
    num_output: 512
    kernel_size: 1
  }
}
layer {
  name: "RDB5_RES"
  type: "Eltwise"
  bottom: "conv6_1"
  bottom: "RDB5_conv4"
  top: "Rconv6_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv6_2"
  type: "Convolution"
  bottom: "Rconv6_1"
  top: "conv6_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu6_2"
  type: "ReLU"
  bottom: "conv6_2"
  top: "conv6_2"
}
layer {
  name: "conv6_3"
  type: "Convolution"
  bottom: "conv6_2"
  top: "conv6_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 2
    dilation: 2
  }
}
layer {
  name: "relu6_3"
  type: "ReLU"
  bottom: "conv6_3"
  top: "conv6_3"
}
layer {
  name: "conv6_3norm"
  type: "BatchNorm"
  bottom: "conv6_3"
  top: "conv6_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv7 *****
# *****************
layer {
  name: "conv7_1"
  type: "Convolution"
  bottom: "conv6_3norm"
  top: "conv7_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu7_1"
  type: "ReLU"
  bottom: "conv7_1"
  top: "conv7_1"
}
layer {
  name: "RDB6_conv1"
  type: "Convolution"
  bottom: "conv7_1"
  top: "RDB6_conv1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "RDB6_relu1"
  type: "ReLU"
  bottom: "RDB6_conv1"
  top: "RDB6_conv1"
}
layer {
  name: "RDB6_concat1"
  bottom: "conv7_1"
  bottom: "RDB6_conv1"
  top: "RDB6_concat1"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB6_conv2"
  type: "Convolution"
  bottom: "RDB6_concat1"
  top: "RDB6_conv2"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "RDB6_relu2"
  type: "ReLU"
  bottom: "RDB6_conv2"
  top: "RDB6_conv2"
}
layer {
  name: "RDB6_concat2"
  bottom: "conv7_1"
  bottom: "RDB6_conv1"
  bottom: "RDB6_conv2"
  top: "RDB6_concat2"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB6_conv3"
  type: "Convolution"
  bottom: "RDB6_concat2"
  top: "RDB6_conv3"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "RDB6_relu3"
  type: "ReLU"
  bottom: "RDB6_conv3"
  top: "RDB6_conv3"
}
layer {
  name: "RDB6_concat3"
  bottom: "conv7_1"
  bottom: "RDB6_conv1"
  bottom: "RDB6_conv2"
  bottom: "RDB6_conv3"
  top: "RDB6_concat3"
  type: "Concat"
  concat_param {
    axis: 1
  }
}
layer {
  name: "RDB6_conv4"
  type: "Convolution"
  bottom: "RDB6_concat3"
  top: "RDB6_conv4"
  convolution_param {
    num_output: 512
    kernel_size: 1
  }
}
layer {
  name: "RDB6_RES"
  type: "Eltwise"
  bottom: "conv7_1"
  bottom: "RDB6_conv4"
  top: "Rconv7_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv7_2"
  type: "Convolution"
  bottom: "Rconv7_1"
  top: "conv7_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu7_2"
  type: "ReLU"
  bottom: "conv7_2"
  top: "conv7_2"
}
layer {
  name: "conv7_3"
  type: "Convolution"
  bottom: "conv7_2"
  top: "conv7_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu7_3"
  type: "ReLU"
  bottom: "conv7_3"
  top: "conv7_3"
}
layer {
  name: "conv7_3norm"
  type: "BatchNorm"
  bottom: "conv7_3"
  top: "conv7_3norm"
  batch_norm_param{ }
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
  param {lr_mult: 0 decay_mult: 0}
}
# *****************
# ***** conv8 *****
# *****************
layer {
  name: "conv8_1"
  type: "Deconvolution"
  bottom: "conv7_3norm"
  top: "conv8_1"
  convolution_param {
    num_output: 256
    kernel_size: 4
    pad: 1
    dilation: 1
    stride: 2
  }
}
layer {
  name: "relu8_1"
  type: "ReLU"
  bottom: "conv8_1"
  top: "conv8_1"
}
layer {
  name: "conv8_2"
  # name: "conv8_2_"
  type: "Convolution"
  bottom: "conv8_1"
  top: "conv8_2"
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu8_2"
  type: "ReLU"
  bottom: "conv8_2"
  top: "conv8_2"
}
layer {
  name: "conv8_3"
  type: "Convolution"
  bottom: "conv8_2"
  top: "conv8_3"
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
    dilation: 1
  }
}
layer {
  name: "relu8_3"
  type: "ReLU"
  bottom: "conv8_3"
  top: "conv8_3"
}
# ****************************
# ***** Unary prediction *****
# ****************************
layer {
  name: "conv8_313"
  type: "Convolution"
  bottom: "conv8_3"
  top: "conv8_313"
  convolution_param {
    num_output: 313
    kernel_size: 1
    stride: 1
    dilation: 1
  }
}
layer {
  name: "conv8_313_rh"
  type: "Scale"
  bottom: "conv8_313"
  top: "conv8_313_rh"
  scale_param {
    bias_term: false
    filler {      type: 'constant'
        value: 0.5
        #2.606
    }
  }
}
layer {
  name: "class8_313_rh"
  type: "Softmax"
  bottom: "conv8_313_rh"
  top: "class8_313_rh"
}
# ********************
# ***** Decoding *****
# ********************
layer {
  name: "class8_ab"
  type: "Convolution"
  bottom: "class8_313_rh"
  top: "class8_ab"
  convolution_param {
    num_output: 2
    kernel_size: 1
    stride: 1
    dilation: 1
  }
}
layer {
  name: "Silence"
  type: "Silence"
  bottom: "class8_ab"
}
